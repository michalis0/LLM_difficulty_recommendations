# Leveraging Large Language Models for Foreign Language Learning: Difficulty Estimation, Topic Estimation and Recommendation

## Description ğŸ¦­

This project contains the code used for our research work described in the article [**Leveraging Large Language Models for Foreign Language Learning: Difficulty Estimation, Topic Estimation and Recommendation**). It contains all the code for our experiments as well as the notebooks used to generate the figures in the article. The aim of the project is to enable :
- The classification of French texts according to their difficulty,
- The classification of French texts according to their subject,
- The recommendation of French texts according to the two previous criteria.
Each of these objectives is dealt with and detailed in a separate folder.

## Installation ğŸ¼

### Requirements ğŸ¨

This project uses the following tools:
- [Pyenv](https://github.com/pyenv/pyenv-installer) : Python version manager
- [Poetry](https://python-poetry.org/docs/#installation) : Python package manager
Tools that you can install using the following commands:
```bash
> curl https://pyenv.run | bash
> curl -sSL https://install.python-poetry.org | python3 -
```

Some notebooks also require the following installations:
- MySQL: [Installation](https://dev.mysql.com/doc/mysql-installation-excerpt/5.7/en/)
- A username and password to connect to the [CURNAGL](https://wiki.unil.ch/ci/books/high-performance-computing-hpc/page/curnagl) cluster at the University of Lausanne used for training the heaviest models. However, this code can be run on a local machine without using the [**Slurmray**](https://github.com/hjamet/SLURM_RAY) package.

### With Pyenv & Poetry ğŸ»

Once you have installed the above tools, you can install the project using the following commands:
```bash
> pyenv install 3.9.13 # Installs the version of Python used in the project
> pyenv local 3.9.13 # Defines the version of Python used in the project
> poetry install # Installs project dependencies
```

### Without Pyenv & Poetry ğŸ™

If you don't want to use Pyenv and Poetry, you can install the project dependencies using the following command:
```bash
> python -m venv .venv # Creates a virtual environment
> source .venv/bin/activate # Activates the virtual environment
> pip install -r requirements.txt # Installs project dependencies
```

## Repository Architecture ğŸ¦¥

The project is structured as follows:
```
.
â”œâ”€â”€ difficulty_estimation (8 notebooks for difficulty estimation)
â”œâ”€â”€ topic_classification (5 notebooks for topic classification)
â”œâ”€â”€ recommendation
â”œâ”€â”€ src (Some frequently used functions and classes)
â”œâ”€â”€ figures (The figures generated by the notebooks and used in the article)
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â””â”€â”€ poetry.lock
```

Notebooks are intended to be executed in alphabetical order. Notebooks in the **"difficulty_estimation "** section are to be executed first, then those in the **"topic_classification "** section and finally those in the **"recommendation "** section.

*Note that executing certain notebooks may cause the following folders to appear:*
```
.
â”œâ”€â”€ data (For data storage)
â”‚ â”œâ”€â”€ raw
â”‚ â”œâ”€â”€ processed
â”œâ”€â”€ results (For storing results)
â”œâ”€â”€ scratch (For storing temporary data)
â”œâ”€â”€ .slogs (If using Slurmray)
```

### Difficulty Estimation ğŸ³

1. `a_Datasets` : This notebook allows you to download the data used to train the models. It also prepares the json files needed to train the OpenAI models and provides a cost estimate.
2. `b_OpenAiModelsTraining` : This notebook allows you to train the OpenAI models.
3. `c_OpenAiEvaluation` : This generates the results and metrics for the OpenAI models.
4. `d_OpenSourceModelsTraining` : This notebook is responsible for training the **CamemBERT** and **Mistral** models.
5. `e_OpenSourceEvaluation` : This generates the results and metrics for the **CamemBERT** and **Mistral** models.
6. `f_PairwiseMismatch` : This notebook trains and evaluates the performance of the **ARI**, **GFI** and **FKGL** Readability Indices. It also introduces the **Pairwise Mismatch** metric and evaluates the performance of all previous models on this metric.
7. `g_CreateFigures` : This notebook generates the figures used in the article.

### Topic Classification ğŸ¬

1. `a_DataPreparation` : Ce notebook se charge du tÃ©lechargement et prÃ©taitement des donnÃ©es utilisÃ©es pour l'entrainement et l'Ã©valuation des modÃ¨les.
2. `b_Zero-Shot` : Ce notebook se charge de l'adaptation d'un modÃ¨le **FlauBERT** prÃ©-entrainÃ© Ã  notre tÃ¢che de classification de sujets. Il Ã©value Ã©galement les performances de ce modÃ¨le ainsi que celles du modÃ¨le Zero-Shot de **mDeBERTa**.
3. `c_OpenAiTopicClassification` : Ce notebook se charge de l'Ã©valuation des performances zero-shot des modÃ¨les OpenAI sur notre tÃ¢che de classification de sujets.
4. `d_FlaubertFineTuned` : Ce notebook vise Ã  entrainer un modÃ¨le **FlauBERT** sur notre tÃ¢che de classification de sujets. Il Ã©value Ã©galement les performances de ce modÃ¨le.
5. `e_CreateFigures` : Ce notebook gÃ©nÃ¨re les figures utilisÃ©es dans l'article.

### Recommendations ğŸ 

The code for the classification experiments is provided in the folder <br> [/recommender_system](recommender_system).
