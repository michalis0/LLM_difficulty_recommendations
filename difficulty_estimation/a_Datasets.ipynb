{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "The purpose of this notebook is to download and analyse the [data](https://drive.switch.ch/index.php/s/W3osjjzrYeqY7mC?path=%2F#editor) used by the difficulty classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- PREPARING NOTEBOOK ---------------------------- #\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Random seed\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# External modules\n",
    "import os\n",
    "from IPython.display import display, Markdown, Latex, clear_output\n",
    "from tqdm import notebook as tqdm\n",
    "\n",
    "# Set global log level\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Define PWD as the current git repository\n",
    "import git\n",
    "repo = git.Repo('.', search_parent_directories=True)\n",
    "pwd = repo.working_dir\n",
    "os.chdir(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DataManager:Initializing data manager...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DataManager:Downloading data from kaggle.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/lopilo/.cache/huggingface/token\n",
      "Login successful\n",
      "Logged in to huggingface hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bcd6ac6eed9468cb468379b110771df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746b13461e82405281b83a4fadb3bd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading Data/ljl_train.csv:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0573109138ff46eda4dd023537e04eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/sentences_train.csv:   0%|          | 0.00/303k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445f659581904e0fa362f466301feefe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)difficulty_train.csv:   0%|          | 0.00/453k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443502caf9914216895ab4cb2dc21b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)a/sentences_test.csv:   0%|          | 0.00/73.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53621798662f4bb68255b4ad4f94ae77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_difficulty_test.csv:   0%|          | 0.00/115k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0e67c9ead040c4acee05fdfa476fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading Data/ljl_test.csv:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DataManager:Moving data to correct folder.\n",
      "WARNING: `git lfs clone` is deprecated and will not be updated\n",
      "          with new flags from `git clone`\n",
      "\n",
      "`git clone` has been updated in upstream Git to have comparable\n",
      "speeds to `git lfs clone`.\n",
      "Cloning into '/home/lopilo/code/Lingorank_LLM/data/raw/zeeguu'...\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- DOWNLOADING DATA ----------------------------- #\n",
    "from src.DataManager import DataManager\n",
    "\n",
    "data_manager = DataManager()\n",
    "data_manager.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>dataset</th>\n",
       "      <th>type_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Est-ce que le professeur Omar est libre ?</td>\n",
       "      <td>A1</td>\n",
       "      <td>french_difficulty</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elle a 21 ans</td>\n",
       "      <td>A1</td>\n",
       "      <td>french_difficulty</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alors comment ça marche ?</td>\n",
       "      <td>A1</td>\n",
       "      <td>french_difficulty</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A bientôt !</td>\n",
       "      <td>A1</td>\n",
       "      <td>french_difficulty</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Le chaufffeur de bus est en retard.</td>\n",
       "      <td>A1</td>\n",
       "      <td>french_difficulty</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>Mais, en thèse logique générale, plus les ques...</td>\n",
       "      <td>C2</td>\n",
       "      <td>sentences</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>Quels que soient les moyens, directs ou indire...</td>\n",
       "      <td>C2</td>\n",
       "      <td>sentences</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>À l'égard des phénomènes vitaux surtout, nonse...</td>\n",
       "      <td>C2</td>\n",
       "      <td>sentences</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>Mais, il ne résul te point inévitablement de c...</td>\n",
       "      <td>C2</td>\n",
       "      <td>sentences</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>Quoi qu'il en soit, ces considérations de haut...</td>\n",
       "      <td>C2</td>\n",
       "      <td>sentences</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7407 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence difficulty  \\\n",
       "0             Est-ce que le professeur Omar est libre ?         A1   \n",
       "1                                         Elle a 21 ans         A1   \n",
       "2                             Alors comment ça marche ?         A1   \n",
       "3                                           A bientôt !         A1   \n",
       "4                   Le chaufffeur de bus est en retard.         A1   \n",
       "...                                                 ...        ...   \n",
       "1915  Mais, en thèse logique générale, plus les ques...         C2   \n",
       "1916  Quels que soient les moyens, directs ou indire...         C2   \n",
       "1917  À l'égard des phénomènes vitaux surtout, nonse...         C2   \n",
       "1918  Mais, il ne résul te point inévitablement de c...         C2   \n",
       "1919  Quoi qu'il en soit, ces considérations de haut...         C2   \n",
       "\n",
       "                dataset type_set  \n",
       "0     french_difficulty    train  \n",
       "1     french_difficulty    train  \n",
       "2     french_difficulty    train  \n",
       "3     french_difficulty    train  \n",
       "4     french_difficulty    train  \n",
       "...                 ...      ...  \n",
       "1915          sentences    train  \n",
       "1916          sentences    train  \n",
       "1917          sentences    train  \n",
       "1918          sentences    train  \n",
       "1919          sentences    train  \n",
       "\n",
       "[7407 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------- DISPLAY DATA ------------------------------- #\n",
    "data_manager.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cannot open 'results/DataManager/train_all_empty_prepared_for_fine_tuning.json' for reading: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "data_manager.get_data_ready_for_fine_tuning(\n",
    "    dataset=\"french_difficulty\",\n",
    "    save = True,\n",
    "    type_set = \"train\",\n",
    "    context = \"CECRL\",\n",
    ")\n",
    "# Display reports/all.yaml\n",
    "!head -n 5 results/DataManager/train_all_empty_prepared_for_fine_tuning.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and analysis for chat model fine-tuning\n",
    "\n",
    "This section serves as a tool to preprocess and analyze the chat dataset used for fine-tuning a chat model. \n",
    "It checks for format errors, provides basic statistics, and estimates token counts for fine-tuning costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken  # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "We first load the chat dataset from an [example JSONL file](https://github.com/openai/openai-cookbook/blob/main/examples/data/toy_chat_fine_tuning.jsonl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 5760\n",
      "First example:\n",
      "{'role': 'system', 'content': ''}\n",
      "{'role': 'user', 'content': 'Robinson comprit bientôt que la première espèce provenait de l’épave de La Virginie et avait proliféré grâce aux réserves de céréales, tandis que l’autre espèce avait toujours vécu dans l’île.'}\n",
      "{'role': 'assistant', 'content': 'A2'}\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(\n",
    "    pwd,\n",
    "    \"data\",\n",
    "    \"processed\",\n",
    "    \"DataManager\",\n",
    "    \"train_all_empty_prepared_for_fine_tuning.json\",\n",
    ")\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format validation\n",
    "\n",
    "We can perform a variety of error checks to validate that each conversation in the dataset adheres to the format expected by the fine-tuning API. Errors are categorized based on their nature for easier debugging.\n",
    "\n",
    "1. **Data Type Check**: Checks whether each entry in the dataset is a dictionary (`dict`). Error type: `data_type`.\n",
    "2. **Presence of Message List**: Checks if a `messages` list is present in each entry. Error type: `missing_messages_list`.\n",
    "3. **Message Keys Check**: Validates that each message in the `messages` list contains the keys `role` and `content`. Error type: `message_missing_key`.\n",
    "4. **Unrecognized Keys in Messages**: Logs if a message has keys other than `role`, `content`, and `name`. Error type: `message_unrecognized_key`.\n",
    "5. **Role Validation**: Ensures the `role` is one of \"system\", \"user\", or \"assistant\". Error type: `unrecognized_role`.\n",
    "6. **Content Validation**: Verifies that `content` has textual data and is a string. Error type: `missing_content`.\n",
    "7. **Assistant Message Presence**: Checks that each conversation has at least one message from the assistant. Error type: `example_missing_assistant_message`.\n",
    "\n",
    "The code below performs these checks, and outputs counts for each type of error found are printed. This is useful for debugging and ensuring the dataset is ready for the next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found errors:\n",
      "missing_content: 5760\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Counting Utilities\n",
    "\n",
    "Lets define a few helpful utilities to be used in the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Warnings and Token Counts \n",
    "\n",
    "With some lightweight analysis we can identify potential issues in the dataset, like missing messages, and provide statistical insights into message and token counts.\n",
    "\n",
    "1. **Missing System/User Messages**: Counts the number of conversations missing a \"system\" or \"user\" message. Such messages are critical for defining the assistant's behavior and initiating the conversation.\n",
    "2. **Number of Messages Per Example**: Summarizes the distribution of the number of messages in each conversation, providing insight into dialogue complexity.\n",
    "3. **Total Tokens Per Example**: Calculates and summarizes the distribution of the total number of tokens in each conversation. Important for understanding fine-tuning costs.\n",
    "4. **Tokens in Assistant's Messages**: Calculates the number of tokens in the assistant's messages per conversation and summarizes this distribution. Useful for understanding the assistant's verbosity.\n",
    "5. **Token Limit Warnings**: Checks if any examples exceed the maximum token limit (4096 tokens), as such examples will be truncated during fine-tuning, potentially resulting in data loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 19, 433\n",
      "mean / median: 51.16336805555556, 43.0\n",
      "p5 / p95: 26.0, 84.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 2, 2\n",
      "mean / median: 2.0, 2.0\n",
      "p5 / p95: 2.0, 2.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(\n",
    "    f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Estimation\n",
    "\n",
    "In this final section, we estimate the total number of tokens that will be used for fine-tuning, which allows us to approximate the cost. It is worth noting that the duration of the fine-tuning jobs will also increase with the token count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~294701 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~884103 tokens\n",
      "Last GPT-3.5 turbo turbo pricing: 7.072824€\n",
      "Last GPT-4 standard pricing: 8.84103€\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(\n",
    "    min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens\n",
    ")\n",
    "print(\n",
    "    f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\"\n",
    ")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(\n",
    "    f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\"\n",
    ")\n",
    "print(\n",
    "    f\"Last GPT-3.5 turbo turbo pricing: {n_epochs * n_billing_tokens_in_dataset * 0.000008}€\"\n",
    ")\n",
    "print(f\"Last GPT-4 standard pricing: {n_billing_tokens_in_dataset * 0.00003}€\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
