{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecommendData\n",
    "\n",
    "The aim of this notebook is to analyse and transform the data from [zeeguu](https://github.com/zeeguu/data-releases.git) so that it can be used by the recommendation model.\n",
    "\n",
    "*First, let's prepare our notebook by importing the necessary libraries and defining the paths to the data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- PREPARING NOTEBOOK ---------------------------- #\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%cd ..\n",
    "\n",
    "# Random seed\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# External modules\n",
    "import os\n",
    "import mysql\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "import time\n",
    "from tqdm import notebook as tqdm\n",
    "import sys\n",
    "\n",
    "# Define PWD as the current git repository\n",
    "import git\n",
    "repo = git.Repo('.', search_parent_directories=True)\n",
    "pwd = repo.working_dir\n",
    "os.chdir(pwd)\n",
    "\n",
    "# Internal modules\n",
    "sys.path.append(pwd)\n",
    "from src.DataManager import DataManager\n",
    "\n",
    "# Set global log level\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download the data\n",
    "\n",
    "Now it's time for us to download the data from **Zeeguu** üòÄ\n",
    "\n",
    "‚ö†Ô∏è **Please note! *Please note that the next two parts of this notebook are particularly long and costly in terms of resources and storage, as they will download the databases, run them on the MySQL server (**which you must have installed**) and finally download them in csv format.* To avoid this waiting time, you can run the next cell, which will download the files already extracted for you. You can then go straight on to part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- DOWNLOADING CSVS FROM HUGGINGFACE -------------------- #\n",
    "# Imports\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Create folder if not exists\n",
    "path = os.path.join(pwd, \"data\", \"processed\")\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Download CSVs\n",
    "snapshot_download(\n",
    "    repo_id=\"OloriBern/FLDE\",\n",
    "    allow_patterns=[\"recommendation/*.csv\"],\n",
    "    local_dir=path,\n",
    "    revision=\"main\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, you can always redo everything manually üôÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- DOWNLOAD DATA ------------------------------ #\n",
    "data_manager = DataManager()\n",
    "data_manager.download(\"recommendation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to send the downloaded files to the zeeguu database in **MySQL**. Make sure you have installed **MySQL** and configured the user:\n",
    "- `user` : zeeguu\n",
    "- password : zeeguu\n",
    "\n",
    "Finally, launch the database with the following command:\n",
    "``bash\n",
    "sudo /etc/init.d/mysql restart\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- PUSH DATA TO MYSQL ---------------------------- #\n",
    "data_manager.push_to_mysql()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform the data into dataframes\n",
    "\n",
    "Now that our data is in the **MySQL** database, we can retrieve it and transform it into dataframes for local storage.\n",
    "\n",
    "**Please note**: Give your *SQL* server time to load the data. You should have *50 tables* in your database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- LIST TABLES ------------------------------- #\n",
    "# D√©finition des identifiants de connection\n",
    "host = \"127.0.0.1\"\n",
    "database = \"zeeguu\"\n",
    "user = \"zeeguu\"\n",
    "password = \"zeeguu\"\n",
    "\n",
    "# Connection √† la base de donn√©es\n",
    "db_connection = mysql.connector.connect(\n",
    "    host=host, user=user, password=password, database=database\n",
    ")\n",
    "cursor = db_connection.cursor()\n",
    "\n",
    "# Liste des tables\n",
    "tables = []\n",
    "display(Markdown(\"*Waiting for tables to be created...*\"))\n",
    "while len(tables) < 50:\n",
    "    cursor.execute(\"SHOW TABLES\")\n",
    "    tables = pd.DataFrame(cursor.fetchall()).iloc[:, 0]\n",
    "    display(\"{}/50 tables created\".format(len(tables)))\n",
    "    print(\"\", end=\"\\r\")\n",
    "    time.sleep(1)\n",
    "\n",
    "display(Markdown(\"### Some tables in the database\"))\n",
    "display(Markdown(\"There are {} tables in the database\".format(len(tables))))\n",
    "display(Markdown(\"Here are some of them:\"))\n",
    "display(tables.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- TRANSFORM DATA TO CSV -------------------------- #\n",
    "# Cr√©ation du dossier de sortie\n",
    "output_folder = os.path.join(pwd, \"data\", \"processed\", \"recommendation\")\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Exportation des tables\n",
    "num_empty_tables = 0\n",
    "for table in tables:\n",
    "    cursor.execute(\"SELECT * FROM {}\".format(table))\n",
    "    value = cursor.fetchall()\n",
    "    if len(value) == 0:\n",
    "        display(Markdown(\"- Table **{}** is empty -> **Skipping...**\".format(table)))\n",
    "        num_empty_tables += 1\n",
    "        continue\n",
    "    df = pd.DataFrame(value)\n",
    "    df.columns = [i[0] for i in cursor.description]\n",
    "    df.to_csv(os.path.join(output_folder, \"{}.csv\".format(table)), index=False)\n",
    "    display(Markdown(\"#### Exported table **{}**\".format(table)))\n",
    "    columns = \", \".join(df.columns)\n",
    "    count = len(df)\n",
    "    display(df.describe().loc[[\"min\", \"max\"]].T)\n",
    "    display(Markdown(\"- **Number of rows**: {}\".format(count)))\n",
    "\n",
    "# Give some information about the export\n",
    "display(\n",
    "    Markdown(\n",
    "        \"**Exportation completed !** (*{}%* of the tables were empty)\".format(\n",
    "            round(num_empty_tables / len(tables) * 100, 2)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Close connection\n",
    "cursor.close()\n",
    "db_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transform to Surprise format\n",
    "\n",
    "Now that we have the data available in the correct format, we'll transform it into a format that can be used by the **Surprise** library.\n",
    "We therefore wish to have a dataframe of the form :\n",
    "\n",
    "| user_id | item_id | rating | timestamp |\n",
    "|---------|---------|--------|-----------|\n",
    "\n",
    "### The rating issue\n",
    "\n",
    "Users don't actually assign a score to the article they're reading. So we need to find a way of assessing the relevance of an article to a user. We're going to explore several avenues:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The reader's estimation of difficulty\n",
    "\n",
    "We have a table containing the difficulties estimated by users for each item. Assuming that :\n",
    "- A **1** is too simple,\n",
    "- A **5** is too difficult,\n",
    "- A **3** is the ideal difficulty\n",
    "\n",
    "\n",
    "We can calculate binary ratings: *2 for correctly estimated difficulty, 1 for wrongly estimated difficulty and 0 for no estimation*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- CREATE SUPRISE DATASET -------------------------- #\n",
    "# Importation des donn√©es\n",
    "df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        pwd,\n",
    "        \"data\",\n",
    "        \"processed\",\n",
    "        \"recommendation\",\n",
    "        \"article_difficulty_feedback.csv\",\n",
    "    ),\n",
    "    index_col=0,\n",
    ")\n",
    "\n",
    "# Mise en forme des donn√©es\n",
    "df = df[[\"user_id\", \"article_id\", \"difficulty_feedback\", \"date\"]]\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"]).astype(int) // 10**9\n",
    "df.sort_values(by=\"date\", inplace=True)\n",
    "df.columns = [\"user_id\", \"article_id\", \"rating\", \"timestamp\"]\n",
    "\n",
    "# Create ratings\n",
    "df[\"rating\"] = (abs(df[\"rating\"] - 3) == 0).astype(int)\n",
    "df[\"rating\"] = df[\"rating\"].replace(2, 1).replace(0, 2)\n",
    "\n",
    "df[\"rating\"][df[\"rating\"] == 2] = 1\n",
    "df[\"rating\"][df[\"rating\"] == 0] = 2\n",
    "\n",
    "# Ajout des 0 pour les utilisateurs qui n'ont pas not√© d'article\n",
    "users = df[\"user_id\"].unique()\n",
    "articles = df[\"article_id\"].unique()\n",
    "all_users = np.repeat(users, len(articles))\n",
    "all_articles = np.tile(articles, len(users))\n",
    "all_ratings = np.zeros(len(all_users))\n",
    "all_timestamps = np.zeros(len(all_users))\n",
    "df_all = pd.DataFrame(\n",
    "    {\n",
    "        \"user_id\": all_users,\n",
    "        \"article_id\": all_articles,\n",
    "        \"rating\": all_ratings,\n",
    "        \"timestamp\": all_timestamps,\n",
    "    }\n",
    ")\n",
    "df = pd.concat([df, df_all], axis=0)\n",
    "\n",
    "# Display and save\n",
    "## Create output folder\n",
    "output_folder = os.path.join(\n",
    "    pwd,\n",
    "    \"results\",\n",
    "    \"recommendation_datasets\",\n",
    ")\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "## Save to csv\n",
    "df.to_csv(os.path.join(output_folder, \"dataset_1.csv\"), index=False, sep=\"\\t\")\n",
    "## Display\n",
    "display(Markdown(\"#### Dataset 1 : *Based on difficulty feedback*\"))\n",
    "display(df.head())\n",
    "## Display some information\n",
    "display(Markdown(\"- **Number of users**: {}\".format(len(df[\"user_id\"].unique()))))\n",
    "display(Markdown(\"- **Number of articles**: {}\".format(len(df[\"article_id\"].unique()))))\n",
    "display(\n",
    "    Markdown(\n",
    "        \"- **Percentage of positive ratings**: {}%\".format(\n",
    "            round(df[\"rating\"] > 0).mean() * 100, 2\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The reading time\n",
    "\n",
    "Now we can use another piece of information provided by our dataset: the time it takes each user to read an article.\n",
    "\n",
    "*‚ö†Ô∏è Please note that the dataset creators themselves have stated that this information is unreliable!*\n",
    "\n",
    "---\n",
    "We'll proceed as follows:\n",
    "- Sum the time spent by each user on each article,\n",
    "- Replace abnormal reading times with minimum or maximum estimated reading times,\n",
    "- Normalize reading times for each user,\n",
    "- Calculate scores based on normalized reading times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- CREATE SURPRISE DATASET ------------------------- #\n",
    "\n",
    "# Importation des donn√©es\n",
    "df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        pwd,\n",
    "        \"data\",\n",
    "        \"processed\",\n",
    "        \"recommendation\",\n",
    "        \"user_reading_session.csv\",\n",
    "    ),\n",
    "    index_col=0,\n",
    ")\n",
    "articles_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        os.getcwd(),\n",
    "        \"Lingorank_LLM\" if not (os.getcwd().endswith(\"Lingorank_LLM\")) else \"\",\n",
    "        \"results\",\n",
    "        \"zeeguu_csvs\",\n",
    "        \"recommendation\",\n",
    "        \"article.csv\",\n",
    "    ),\n",
    "    index_col=0,\n",
    ")\n",
    "\n",
    "# Mise en forme des donn√©es\n",
    "df = df[[\"user_id\", \"article_id\", \"duration\", \"start_time\"]]\n",
    "df.rename(columns={\"start_time\": \"timestamp\"}, inplace=True)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"]).astype(int) // 10**9\n",
    "df = df.merge(articles_df[\"word_count\"], left_on=\"article_id\", right_index=True)\n",
    "df = df[df[\"word_count\"].str.isnumeric().astype(bool)].dropna(subset=[\"word_count\"])\n",
    "df[\"word_count\"] = df[\"word_count\"].astype(int)\n",
    "df[\"estimated_reading_time\"] = df[\"word_count\"] / 150  # 150 words per minute\n",
    "df[\"duration\"] = df[\"duration\"] / 60e3\n",
    "df.drop(columns=[\"word_count\"], inplace=True)\n",
    "df[\"article_id\"] = df[\"article_id\"].astype(int)\n",
    "\n",
    "# Somme des temps de lecture par utilisateur et par article\n",
    "df = df.groupby([\"user_id\", \"article_id\"]).sum().reset_index()\n",
    "\n",
    "# Cr√©ation des scores\n",
    "df[\"rating\"] = (df[\"duration\"] / df[\"estimated_reading_time\"]).clip(\n",
    "    upper=1, lower=0\n",
    ") + 1\n",
    "df.drop(columns=[\"duration\", \"estimated_reading_time\"], inplace=True)\n",
    "\n",
    "# Ajout des 0 pour les utilisateurs qui n'ont pas not√© d'article\n",
    "users = df[\"user_id\"].unique()\n",
    "articles = df[\"article_id\"].unique()\n",
    "all_users = np.repeat(users, len(articles))\n",
    "all_articles = np.tile(articles, len(users))\n",
    "all_ratings = np.zeros(len(all_users))\n",
    "df_all = pd.DataFrame(\n",
    "    {\n",
    "        \"user_id\": all_users,\n",
    "        \"article_id\": all_articles,\n",
    "        \"rating\": all_ratings,\n",
    "        \"timestamp\": 0,\n",
    "    }\n",
    ")\n",
    "df = pd.concat([df, df_all], axis=0)\n",
    "\n",
    "# Save and display\n",
    "## Create output folder\n",
    "output_folder = os.path.join(\n",
    "    pwd,\n",
    "    \"results\",\n",
    "    \"recommendation_datasets\",\n",
    ")\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "## Save to csv\n",
    "df.to_csv(os.path.join(output_folder, \"dataset_2.csv\"), index=False, sep=\"\\t\")\n",
    "## Display\n",
    "display(Markdown(\"#### Dataset 2 : *Based on reading session*\"))\n",
    "display(df)\n",
    "## Display some information\n",
    "display(Markdown(\"- **Number of users**: {}\".format(len(df[\"user_id\"].unique()))))\n",
    "display(Markdown(\"- **Number of articles**: {}\".format(len(df[\"article_id\"].unique()))))\n",
    "display(\n",
    "    Markdown(\n",
    "        \"- **Percentage of positive ratings**: {}%\".format(\n",
    "            round(df[\"rating\"] > 0).mean() * 100, 2\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the article\n",
    "\n",
    "Finally, in order to obtain what will certainly be our most extensive dataset, we'll create a dataset by applying the following evaluation strategy:\n",
    "- **2** if the user has read and liked the article,\n",
    "- **1** if the user has read the article,\n",
    "- **0** otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- CREATE SURPRISE DATASET ------------------------- #\n",
    "\n",
    "# Importation des donn√©es\n",
    "df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        pwd,\n",
    "        \"data\",\n",
    "        \"processed\",\n",
    "        \"recommendation\",\n",
    "        \"user_article.csv\",\n",
    "    ),\n",
    "    index_col=0,\n",
    ")\n",
    "\n",
    "# Mise en forme des donn√©es\n",
    "df = df[[\"user_id\", \"article_id\", \"liked\", \"opened\"]].dropna()\n",
    "df.rename(columns={\"opened\": \"timestamp\"}, inplace=True)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"]).astype(int) // 10**9\n",
    "df[\"article_id\"] = df[\"article_id\"].astype(int)\n",
    "df[\"rating\"] = df[\"liked\"].astype(int) + 1\n",
    "df.drop(columns=[\"liked\"], inplace=True)\n",
    "df = df[[\"user_id\", \"article_id\", \"rating\", \"timestamp\"]]\n",
    "\n",
    "# Ajout des 0 pour les utilisateurs qui n'ont pas not√© d'article\n",
    "users = df[\"user_id\"].unique()\n",
    "articles = df[\"article_id\"].unique()\n",
    "all_users = np.repeat(users, len(articles))\n",
    "all_articles = np.tile(articles, len(users))\n",
    "all_ratings = np.zeros(len(all_users))\n",
    "all_timestamps = np.zeros(len(all_users))\n",
    "df_all = pd.DataFrame(\n",
    "    {\n",
    "        \"user_id\": all_users,\n",
    "        \"article_id\": all_articles,\n",
    "        \"rating\": all_ratings,\n",
    "        \"timestamp\": all_timestamps,\n",
    "    }\n",
    ")\n",
    "df = pd.concat([df, df_all], axis=0)\n",
    "\n",
    "# Save and display\n",
    "## Create output folder\n",
    "output_folder = os.path.join(\n",
    "    pwd,\n",
    "    \"results\",\n",
    "    \"recommendation_datasets\",\n",
    ")\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "## Save to csv\n",
    "df.to_csv(os.path.join(output_folder, \"dataset_3.csv\"), index=False, sep=\"\\t\")\n",
    "## Display\n",
    "display(Markdown(\"#### Dataset 3 : *Based on user article*\"))\n",
    "display(df)\n",
    "## Display some information\n",
    "display(Markdown(\"- **Number of users**: {}\".format(len(df[\"user_id\"].unique()))))\n",
    "display(Markdown(\"- **Number of articles**: {}\".format(len(df[\"article_id\"].unique()))))\n",
    "display(\n",
    "    Markdown(\n",
    "        \"- **Percentage of positive ratings**: {}%\".format(\n",
    "            round(df[\"rating\"] > 0).mean() * 100, 2\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
